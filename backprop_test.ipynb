{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.hid1 = torch.nn.Linear(3,3) \n",
    "    self.hid2 = torch.nn.Linear(3,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = torch.relu(self.hid1(x))\n",
    "    z = torch.relu(self.hid2(z))\n",
    "    return z\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2077,  0.1458,  0.0724],\n",
      "        [ 0.3559,  0.0342,  0.5717],\n",
      "        [-0.1638,  0.3048,  0.1396]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1399, -0.4112,  0.0286], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0470, -0.5717, -0.5683]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3677], requires_grad=True)\n",
      "tensor([0.], grad_fn=<ReluBackward0>)\n",
      "tensor(16., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y, y_hat):\n",
    "  return torch.mean((y - y_hat)**2)\n",
    "# Create a tensor\n",
    "input = torch.tensor([1.0, 2.0, 3.0])\n",
    "target = torch.tensor([4.0])\n",
    "model = Net()\n",
    "print(model.hid1.weight)\n",
    "print(model.hid1.bias)\n",
    "print(model.hid2.weight)\n",
    "print(model.hid2.bias)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss = loss_function\n",
    "output = model(input)\n",
    "print(output)\n",
    "loss_value = loss(output, target)\n",
    "print(loss_value)\n",
    "loss_value.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/gclyne/thesis/old_code/ann/backprop_test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gclyne/thesis/old_code/ann/backprop_test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(outputs\u001b[39m=\u001b[39;49moutput, inputs\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/thesis/env/lib/python3.9/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(outputs=output, inputs=input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([[0., 0., 0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "print(model.hid1.weight.grad)\n",
    "print(model.hid1.bias.grad)\n",
    "print(model.hid2.weight.grad)\n",
    "print(model.hid2.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2479, -0.1212, -1.9611],\n",
      "        [ 1.0857, -1.5598,  1.6598]], requires_grad=True)\n",
      "tensor([ 0.8378, -1.6809, -0.3013], grad_fn=<SqueezeBackward3>)\n",
      "tensor([-0.2479, -0.1212, -1.9611], grad_fn=<SelectBackward0>)\n",
      "tensor([0.7019, 2.8255, 0.0908], grad_fn=<PowBackward0>)\n",
      "tensor([1.1736, 6.0627])\n",
      "tensor([[ 1.6755, -3.3619, -0.6026],\n",
      "        [ 1.6755, -3.3619, -0.6026]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,requires_grad=True)  # input tensor\n",
    "y = torch.zeros(3,requires_grad=True)  # expected output\n",
    "W = torch.randn(2, 3, requires_grad=True) # weights\n",
    "# b = torch.randn(3, requires_grad=True) # bias vector\n",
    "z = torch.matmul(x,W) # output\n",
    "loss = (z - y).pow(2)\n",
    "print(W)\n",
    "# print(b)\n",
    "print(z)\n",
    "print(W[0])\n",
    "# print(W[0].sum())\n",
    "# print(W[1].sum())\n",
    "# print(W[0].sum() * 2 * W[0].sum())\n",
    "# print(2 * W[0].sum() * (z ))\n",
    "print(loss)\n",
    "external_grad = torch.tensor([1.,1.,1.,])\n",
    "loss.backward(gradient=external_grad)\n",
    "print(x.grad)\n",
    "print(W.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4154, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * z[0]  * (W[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6755, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2960, -0.4942, -0.8776],\n",
      "        [-0.2960, -0.4942, -0.8776]])\n",
      "tensor([-0.2960, -0.4942, -0.8776])\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/_wddvqdx1zn1sqjjw3s9m2940000gt/T/ipykernel_22312/63881608.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  print(z.grad) # WARNING\n",
      "/var/folders/1g/_wddvqdx1zn1sqjjw3s9m2940000gt/T/ipykernel_22312/63881608.py:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  print(loss.grad) # WARNING\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(W.grad) #OK\n",
    "print(b.grad) #OK\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad) # WARNING\n",
    "print(loss.grad) # WARNING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "268ec44076fd2df7fec26bbac46d7be918e1f9b37934d455d4264e66a92714db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
